<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Bridge v3 - Phone Dialer</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 800px;
      margin: 50px auto;
      padding: 20px;
      background: #f5f5f5;
    }

    .container {
      background: white;
      padding: 30px;
      border-radius: 8px;
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
    }

    h1 {
      color: #333;
      margin-bottom: 20px;
    }

    .status {
      padding: 15px;
      margin: 10px 0;
      border-radius: 4px;
      font-size: 14px;
    }

    .status.info {
      background: #e3f2fd;
      border-left: 4px solid #2196F3;
    }

    .status.error {
      background: #ffebee;
      border-left: 4px solid #f44336;
    }

    .status.success {
      background: #e8f5e9;
      border-left: 4px solid #4caf50;
    }

    .call-info {
      margin-top: 20px;
      padding: 15px;
      background: #fafafa;
      border-radius: 4px;
    }

    .call-info p {
      margin: 5px 0;
    }

    .audio-monitor {
      margin-top: 20px;
      padding: 20px;
      background: #fff;
      border-radius: 8px;
      border: 2px solid #e0e0e0;
    }

    .audio-monitor h3 {
      margin-top: 0;
      font-size: 16px;
      color: #333;
    }

    .audio-level {
      margin: 15px 0;
    }

    .audio-level-label {
      font-size: 14px;
      margin-bottom: 5px;
      font-weight: bold;
    }

    .audio-level-bar {
      height: 30px;
      background: #e0e0e0;
      border-radius: 15px;
      overflow: hidden;
      position: relative;
    }

    .audio-level-fill {
      height: 100%;
      background: linear-gradient(to right, #4caf50, #8bc34a, #ffeb3b, #ff9800, #f44336);
      width: 0%;
      transition: width 0.1s ease-out;
    }

    .audio-level-value {
      position: absolute;
      right: 10px;
      top: 50%;
      transform: translateY(-50%);
      font-size: 12px;
      font-weight: bold;
      color: #333;
    }

    .audio-status {
      display: inline-block;
      width: 12px;
      height: 12px;
      border-radius: 50%;
      background: #ccc;
      margin-left: 10px;
    }

    .audio-status.active {
      background: #4caf50;
      animation: pulse 1s infinite;
    }

    @keyframes pulse {

      0%,
      100% {
        opacity: 1;
      }

      50% {
        opacity: 0.5;
      }
    }
  </style>
</head>

<body>
  <div class="container">
    <h1>üìû Bridge v3 - Phone Dialer</h1>
    
    <button id="start-button"
      style="padding: 15px 30px; font-size: 16px; background: #4caf50; color: white; border: none; border-radius: 4px; cursor: pointer; margin-bottom: 20px;">
      üöÄ START BRIDGE (Click to enable audio)
    </button>
    
    <div class="call-info">
      <p><strong>Calling:</strong> <span id="numberToCall">+46737606800</span></p>
      <p><strong>From:</strong> <span id="numberToCallFrom">+46775893847</span></p>
    </div>
    
    <div class="audio-monitor">
      <h3>üéµ Audio & Transcription Monitor</h3>
      <p style="font-size: 12px; color: #666; margin: 5px 0 15px 0;">
        <strong>NEW Architecture:</strong> Phone ‚Üí Soniox STT ‚Üí ElevenLabs (text) ‚Üí Phone<br>
        <em>Phone audio ‚Üí Soniox transcription ‚Üí AI text response ‚Üí TTS audio output</em>
      </p>
      
      <div class="audio-level">
        <div class="audio-level-label">
          üé§ Phone Audio ‚Üí Soniox STT
          <span class="audio-status" id="phone-to-agent-status"></span>
        </div>
        <div class="audio-level-bar">
          <div class="audio-level-fill" id="phone-to-agent-level"></div>
          <div class="audio-level-value" id="phone-to-agent-value">0%</div>
        </div>
      </div>
      
      <div class="audio-level">
        <div class="audio-level-label">
          üîä AI TTS Audio ‚Üí Remote Party
          <span class="audio-status" id="agent-to-phone-status"></span>
        </div>
        <div class="audio-level-bar">
          <div class="audio-level-fill" id="agent-to-phone-level"></div>
          <div class="audio-level-value" id="agent-to-phone-value">0%</div>
        </div>
      </div>
    </div>
    
    <div id="status-container"></div>
  </div>

  <script src="https://app.salesys.se/libs/easytelecom/jquery-2.1.1.min.js"></script>
  <script src="https://app.salesys.se/libs/easytelecom/jquery.json-2.4.min.js"></script>
  <script src="https://app.salesys.se/libs/easytelecom/verto-min-0.0.2.js"></script>
  <script src="https://app.salesys.se/libs/easytelecom/media-device-id.min.js"></script>
  <script src="/virtual-audio-source.js"></script>
  <!-- Use custom Soniox implementation with CORRECT API -->
  <script src="/soniox-stt.js"></script>
  <script src="/elevenlabs-bridge.js"></script>
  <audio autoplay hidden id="audio-stream"></audio>


  <script defer>
    const salesysToken = "6641c32423059c59f0ebeec3-A3lhQHWLdVSR2e4WXr6f2SuTK9rSisnhaYz5XuRoDsn8TUmqpBATwbZRWwCJbmjjtXC7LpN42c8cIK5bu3B9ObIx1vmQx2Cgmx8+E3SJcmuqQdKX7+qg6h8Z2ewHzmZZzHspSHatgGxr+F3o2+iqA9n957Gtw2VGbs8d3DfxgRI=";
    const telnectToken = "NnoXdRRU8hAvoFVfxxo2JwyF80ukM5rF0rcAksJl";

    const numberToCall = "+46737606800";
    const numberToCallFrom = "+46775893847";

    // ElevenLabs configuration
    const elevenLabsConfig = {
      agentId: 'agent_5901k6ecb9epfwhrn82qgkhg0qtw',
      apiKey: 'sk_76e79a417321b8856248e1a6d8b50cabc92463b528727e34'
    };

    // Soniox STT configuration
    const sonioxConfig = {
      apiKey: '6d2e14f6a742cfaef685a06fcee55941868e544eae32a805090a37b4d04510d0'
    };

    // Thinking sounds configuration
    const thinkingSounds = {
      folder: '/tics',  // Use existing tics folder
      files: ['tic1.mp3', 'tic2.mp3', 'tic3.mp3'],// Your existing sound files
      enabled: true,
      probability: 0.5 // 50% chance to play sound
    };

    // Interrupt sounds configuration
    const interruptSounds = {
      folder: '/interrupt',
      files: ['int1.mp3', 'int2.mp3', 'int3.mp3'], // Add your interrupt sounds
      enabled: true
    };

    // Telnect recording configuration
    const telnectConfig = {
      apiUrl: '/proxy/telnect/api/v1', // Use proxy to avoid CORS
      bearerToken: 'NnoXdRRU8hAvoFVfxxo2JwyF80ukM5rF0rcAksJl'
    };

    let elevenLabsBridge = null;
    let sonioxSTT = null; // NEW: Soniox STT instance
    let phoneAudioStream = null;
    let virtualAudioSource = null;
    let currentCallId = null; // Store the call ID for recording control
    
    // Use WINDOW scope for interruption state (accessible everywhere)
    window.isAISpeaking = false; // Track if AI is currently speaking
    window.userInterruptionDetected = false; // Track if user started speaking during AI
    window.aiSpeakingStartTime = null; // Track when AI started current response (for analytics)
    
    // ADAPTIVE INTERRUPTION SENSITIVITY
    let interruptionDebounceTimer = null;
    
    // Analytics tracking
    const interruptionAnalytics = {
      interrupts: [], // Array of {timestamp, aiDuration}
      currentSensitivity: 'normal', // normal, medium, strict
      
      // Sensitivity thresholds
      thresholds: {
        normal: { debounce: 400, minWords: 1 },
        medium: { debounce: 550, minWords: 2 },
        strict: { debounce: 700, minWords: 3 }
      },
      
      // Track an interruption
      record(aiDuration) {
        const now = Date.now();
        this.interrupts.push({ timestamp: now, aiDuration });
        
        // Clean old entries (>2 minutes)
        this.interrupts = this.interrupts.filter(i => now - i.timestamp < 120000);
        
        console.log('[Adaptive] Recorded interrupt - AI had spoken for', aiDuration, 'ms');
        this.evaluateSensitivity();
      },
      
      // Evaluate and adjust sensitivity
      evaluateSensitivity() {
        const now = Date.now();
        const recentInterrupts = this.interrupts.filter(i => now - i.timestamp < 60000);
        
        if (recentInterrupts.length === 0) {
          // No recent interrupts - reset to normal
          if (this.currentSensitivity !== 'normal') {
            console.log('[Adaptive] ‚úÖ No interrupts for 1min - resetting to NORMAL sensitivity');
            this.currentSensitivity = 'normal';
          }
          return;
        }
        
        // Count early interrupts (AI spoke <2 seconds)
        const earlyInterrupts = recentInterrupts.filter(i => i.aiDuration < 2000).length;
        
        console.log('[Adaptive] Analysis: ', recentInterrupts.length, 'interrupts in last 60s,', earlyInterrupts, 'were early');
        
        // Adjust sensitivity based on patterns
        if (earlyInterrupts >= 3 && this.currentSensitivity === 'normal') {
          this.currentSensitivity = 'medium';
          console.log('[Adaptive] ‚ö†Ô∏è Too many early interrupts - increasing to MEDIUM sensitivity');
        } else if (earlyInterrupts >= 5 && this.currentSensitivity === 'medium') {
          this.currentSensitivity = 'strict';
          console.log('[Adaptive] ‚ö†Ô∏è‚ö†Ô∏è Excessive early interrupts - increasing to STRICT sensitivity');
        } else if (earlyInterrupts <= 1 && this.currentSensitivity === 'strict') {
          this.currentSensitivity = 'medium';
          console.log('[Adaptive] ‚úÖ Fewer false positives - reducing to MEDIUM sensitivity');
        } else if (earlyInterrupts === 0 && this.currentSensitivity === 'medium') {
          this.currentSensitivity = 'normal';
          console.log('[Adaptive] ‚úÖ No false positives - resetting to NORMAL sensitivity');
        }
        
        const config = this.thresholds[this.currentSensitivity];
        console.log('[Adaptive] Current config:', this.currentSensitivity, '- debounce:', config.debounce + 'ms', 'minWords:', config.minWords);
      },
      
      // Get current configuration
      getConfig() {
        return this.thresholds[this.currentSensitivity];
      }
    };
    
    // Audio level tracking for decay
    let lastPhoneLevel = 0;
    let lastAgentLevel = 0;
    
    // Pre-create audio context for synthetic stream (avoid user gesture requirement)
    let syntheticStreamContext = null;
    let syntheticStreamDestination = null;

    // Create virtual audio source (this will be the "microphone" for AI audio)
    // Using 48kHz to match both phone system and ElevenLabs
    virtualAudioSource = new VirtualAudioSource(48000);

    // Helper function to play a random sound (thinking or interrupt)
    async function playRandomSound(config, label) {
      console.log(`[${label}] üé¨ playRandomSound called - enabled:`, config.enabled, 'virtualAudio:', !!virtualAudioSource);
      
      if (!config.enabled || !virtualAudioSource) {
        console.log(`[${label}] ‚ùå Sounds disabled or no virtual audio`);
        return;
      }

      // Check probability if it exists
      if (config.probability !== undefined) {
        const roll = Math.random();
        if (roll > config.probability) {
          console.log(`[${label}] Skipped (probability - rolled`, roll.toFixed(2), '>', config.probability + ')');
          return;
        }
        console.log(`[${label}] ‚úÖ Playing (rolled`, roll.toFixed(2), '<=', config.probability + ')');
      }

      try {
        // Pick random sound file
        const randomIndex = Math.floor(Math.random() * config.files.length);
        const soundFile = config.files[randomIndex];
        const soundUrl = `${config.folder}/${soundFile}`;

        console.log(`[${label}] üéµ Fetching sound:`, soundUrl);
        addStatus(`${label === 'Thinking' ? 'ü§î' : 'üõë'} Playing ${label.toLowerCase()} sound...`, 'info');

        // Fetch and decode the audio file
        const response = await fetch(soundUrl);
        if (!response.ok) {
          throw new Error(`HTTP ${response.status}: ${response.statusText}`);
        }
        
        console.log(`[${label}] ‚úÖ Fetched, decoding...`);
        const arrayBuffer = await response.arrayBuffer();

        // Decode audio
        const audioContext = new AudioContext({ sampleRate: 48000 });
        const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

        console.log(`[${label}] ‚úÖ Decoded - Duration:`, audioBuffer.duration, 's, Samples:', audioBuffer.length);

        // Convert to PCM16 and send to virtual audio source
        const channelData = audioBuffer.getChannelData(0); // Mono
        const pcm16 = new Int16Array(channelData.length);

        for (let i = 0; i < channelData.length; i++) {
          const s = Math.max(-1, Math.min(1, channelData[i]));
          pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
        }

        // Send to virtual audio (caller will hear it)
        virtualAudioSource.addAudioData(pcm16);
        console.log(`[${label}] ‚úÖ‚úÖ‚úÖ Sound sent to caller - PCM samples:`, pcm16.length);

      } catch (error) {
        console.error(`[${label}] ‚ùå‚ùå‚ùå Failed to play sound:`, error);
        console.error(`[${label}] Error details:`, error.message, error.stack);
      }
    }

    // Wrapper functions
    async function playThinkingSound() {
      await playRandomSound(thinkingSounds, 'Thinking');
    }

    async function playInterruptSound() {
      await playRandomSound(interruptSounds, 'Interrupt');
    }
    
    // Pre-create the synthetic stream context to avoid user gesture issues
    try {
      syntheticStreamContext = new AudioContext({ sampleRate: 48000 });
      syntheticStreamDestination = syntheticStreamContext.createMediaStreamDestination();
      console.log('[Bridge] Pre-created synthetic stream context for headless operation');
    } catch (e) {
      console.warn('[Bridge] Could not pre-create context:', e);
    }
    
    // Add initial silence to keep stream active
    virtualAudioSource.addSilence(100);
    setInterval(() => {
      if (virtualAudioSource && !virtualAudioSource.playing) {
        virtualAudioSource.addSilence(50); // Keep alive with silence
      }
    }, 5000);
    
    addStatus('‚úì Virtual audio source created for AI‚ÜíPhone bridge', 'success');

    // Check if running in headless mode or inbound call mode
    const urlParams = new URLSearchParams(window.location.search);
    const isHeadless = urlParams.get('headless') === 'true';
    const inboundCallId = urlParams.get('callId');
    const isInbound = urlParams.get('inbound') === 'true';
    
    // Store inbound call ID if provided
    if (inboundCallId && isInbound) {
      window.inboundCallId = inboundCallId;
      console.log('[Bridge] üìû INBOUND CALL MODE - Call ID:', inboundCallId);
    }
    
    if (isHeadless || (inboundCallId && isInbound)) {
      console.log('[Bridge] ü§ñ Running in HEADLESS/INBOUND mode - auto-resuming contexts...');
      
      // Auto-resume AudioContexts (no user gesture needed in Puppeteer)
      setTimeout(async () => {
        try {
          if (virtualAudioSource && virtualAudioSource.audioContext) {
            await virtualAudioSource.audioContext.resume();
            console.log('[Bridge] ‚úÖ Virtual audio context auto-resumed');
          }
          if (syntheticStreamContext) {
            await syntheticStreamContext.resume();
            console.log('[Bridge] ‚úÖ Synthetic stream context auto-resumed');
          }
          
          // Auto-start bridge for inbound calls
          if (inboundCallId && isInbound) {
            console.log('[Bridge] üöÄ Auto-starting bridge for inbound call...');
            startBridge();
          } else {
            // Signal that bridge is ready for call injection
            window.bridgeReady = true;
            console.log('[Bridge] ‚úÖ Bridge ready for call injection');
          }
        } catch (e) {
          console.error('[Bridge] Failed to auto-resume:', e);
        }
      }, 1000);
    }

    // Store reference to real microphone stream when Verto requests it
    let vertoMicrophoneStream = null;
    let phoneToAIStream = null; // Synthetic stream for bridging phone to AI
    
    // Monkey-patch getUserMedia for HEADLESS operation
    const originalGetUserMedia = navigator.mediaDevices.getUserMedia.bind(navigator.mediaDevices);
    let getUserMediaCallCount = 0;
    navigator.mediaDevices.getUserMedia = async function (constraints) {
      getUserMediaCallCount++;
      console.log('[Bridge] getUserMedia intercepted - Call #' + getUserMediaCallCount, constraints);
      
      if (constraints && constraints.audio) {
        // First call is from Verto (for output) - give it virtual audio
        if (getUserMediaCallCount === 1) {
          console.log('[Bridge] ‚Üí Call #1 (Verto): Returning VIRTUAL audio source (AI‚ÜíPhone)');
        addStatus('üì° Verto: using virtual audio (AI‚ÜíPhone)', 'info');
        return virtualAudioSource.getMediaStream();
        }
        // Second call is from Soniox SDK (for input) - give it phone audio
        else if (getUserMediaCallCount === 2 && phoneAudioStream) {
          console.log('[Bridge] ‚Üí Call #2 (Soniox SDK): Returning PHONE audio stream (Phone‚ÜíSTT)');
          addStatus('üé§ Soniox SDK: using phone audio stream', 'info');
          return phoneAudioStream;
        }
        // Fallback
        else {
          console.log('[Bridge] ‚Üí Call #' + getUserMediaCallCount + ': Returning virtual audio (default)');
          return virtualAudioSource.getMediaStream();
        }
      }
      
      return originalGetUserMedia(constraints);
    };

    // Helper function to add status messages
    function addStatus(message, type = 'info') {
      const container = document.getElementById('status-container');
      const statusDiv = document.createElement('div');
      statusDiv.className = `status ${type}`;
      statusDiv.innerHTML = `<strong>${new Date().toLocaleTimeString()}</strong> - ${message}`;
      container.appendChild(statusDiv);
      console.log(`[${type.toUpperCase()}] ${message}`);
    }

    // Helper function to update audio levels
    function updateAudioLevel(direction, level) {
      const fillElement = document.getElementById(`${direction}-level`);
      const valueElement = document.getElementById(`${direction}-value`);
      const statusElement = document.getElementById(`${direction}-status`);
      
      if (fillElement && valueElement && statusElement) {
        fillElement.style.width = `${level}%`;
        valueElement.textContent = `${level}%`;
        
        // Update status indicator (active if level > 5%)
        if (level > 5) {
          statusElement.classList.add('active');
        } else {
          statusElement.classList.remove('active');
        }
      }
      
      // Store last levels
      if (direction === 'phone-to-agent') {
        lastPhoneLevel = level;
      } else if (direction === 'agent-to-phone') {
        lastAgentLevel = level;
      }
    }
    
    // Decay audio levels over time for smooth visualization
    setInterval(() => {
      // Decay phone level
      if (lastPhoneLevel > 0) {
        lastPhoneLevel = Math.max(0, lastPhoneLevel - 3);
        const fillElement = document.getElementById('phone-to-agent-level');
        const valueElement = document.getElementById('phone-to-agent-value');
        if (fillElement && valueElement) {
          fillElement.style.width = `${lastPhoneLevel}%`;
          valueElement.textContent = `${lastPhoneLevel}%`;
        }
      }
      
      // Decay agent level
      if (lastAgentLevel > 0) {
        lastAgentLevel = Math.max(0, lastAgentLevel - 3);
        const fillElement = document.getElementById('agent-to-phone-level');
        const valueElement = document.getElementById('agent-to-phone-value');
        if (fillElement && valueElement) {
          fillElement.style.width = `${lastAgentLevel}%`;
          valueElement.textContent = `${lastAgentLevel}%`;
        }
      }
    }, 100); // Update every 100ms

    // Helper function to start call recording via Telnect API
    async function startRecording(callId) {
      if (!callId) {
        console.error('[Recording] No call ID provided');
        return;
      }

      try {
        addStatus(`üî¥ Starting recording for call ${callId}...`, 'info');
        
        const response = await fetch(`${telnectConfig.apiUrl}/Calls/${callId}`, {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${telnectConfig.bearerToken}`,
            'Content-Type': 'application/json'
          },
          body: JSON.stringify({
            actions: [
              {
                action: 'recording_start'
              }
            ]
          })
        });

        if (response.ok) {
          const result = await response.json();
          addStatus('‚úì Recording started successfully', 'success');
          console.log('[Recording] Started for call:', callId, 'Response:', result);
        } else {
          const errorText = await response.text();
          addStatus(`‚ö†Ô∏è Failed to start recording: ${response.status}`, 'error');
          console.error('[Recording] Error:', response.status, errorText);
          
          // Try to parse error as JSON for better debugging
          try {
            const errorJson = JSON.parse(errorText);
            console.error('[Recording] Error details:', errorJson);
            addStatus(`Error details: ${JSON.stringify(errorJson)}`, 'error');
          } catch (e) {
            // Not JSON, already logged as text
          }
        }
      } catch (error) {
        addStatus(`‚ùå Recording error: ${error.message}`, 'error');
        console.error('[Recording] Exception:', error);
      }
    }

    // Helper function to stop call recording via Telnect API
    async function stopRecording(callId) {
      if (!callId) {
        console.error('[Recording] No call ID provided');
        return;
      }

      try {
        addStatus(`‚èπÔ∏è Stopping recording for call ${callId}...`, 'info');
        
        const response = await fetch(`${telnectConfig.apiUrl}/Calls/${callId}`, {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${telnectConfig.bearerToken}`,
            'Content-Type': 'application/json'
          },
          body: JSON.stringify({
            actions: [{
              action: 'recording_stop'
            }]
          })
        });

        if (response.ok) {
          addStatus('‚úì Recording stopped successfully', 'success');
          console.log('[Recording] Stopped for call:', callId);
        } else {
          const errorText = await response.text();
          addStatus(`‚ö†Ô∏è Failed to stop recording: ${response.status}`, 'error');
          console.error('[Recording] Error:', response.status, errorText);
        }
      } catch (error) {
        addStatus(`‚ùå Recording stop error: ${error.message}`, 'error');
        console.error('[Recording] Exception:', error);
      }
    }

    addStatus('üëÜ Click START BRIDGE button to begin', 'info');
    
    // Button click handler to start everything (provides user gesture)
    document.getElementById('start-button').addEventListener('click', async () => {
      const button = document.getElementById('start-button');
      button.disabled = true;
      button.textContent = '‚è≥ Starting...';
      button.style.background = '#999';
      
      addStatus('Starting Bridge v3...', 'info');
      addStatus('Resuming audio contexts...', 'info');
      
      // Resume all audio contexts with user gesture
      try {
        if (virtualAudioSource && virtualAudioSource.audioContext) {
          await virtualAudioSource.audioContext.resume();
          console.log('[Bridge] Virtual audio context resumed');
        }
        if (syntheticStreamContext) {
          await syntheticStreamContext.resume();
          console.log('[Bridge] Synthetic stream context resumed');
        }
        addStatus('‚úì Audio contexts active', 'success');
      } catch (e) {
        console.warn('[Bridge] Could not resume contexts:', e);
      }
      
      button.textContent = '‚úÖ Bridge Active';
      button.style.background = '#4caf50';
      
      startBridge();
    });
    
    function startBridge() {
      addStatus('Attempting to login to Salesys API...', 'info');

    // Use proxy to avoid CORS issues
    fetch("/proxy/api/dial/easytelecom-v1/login", {
      method: "POST",
      headers: {
        "Authorization": "Bearer " + salesysToken
      }
    }).then(res => {
      if (!res.ok) {
        throw new Error(`Login failed: ${res.status} ${res.statusText}`);
      }
      return res.json();
    }).then(login => {
      addStatus('‚úì Successfully logged in to Salesys', 'success');
      addStatus('Initializing Verto connection...', 'info');
      
      const verto = new $.verto({
        login: `${login.username}@${login.domain}`,
        passwd: login.password,
        iceServers: true,
        socketUrl: login.url,
        tag: "audio-stream"
      }, {
        onRemoteStream: stream => {
            console.log("========== onRemoteStream FIRED ==========");
            console.log('[Audio] Stream ID:', stream.id);
            console.log('[Audio] Active:', stream.active);
            console.log('[Audio] Audio tracks:', stream.getAudioTracks().length);
            console.log('[Audio] Is inbound call:', !!window.inboundCallId);
            console.log('[Audio] Already initialized:', { elevenlabs: !!elevenLabsBridge, soniox: !!sonioxSTT });

          addStatus('üìû Incoming audio stream from remote party', 'success');
          
            // ALWAYS set phoneAudioStream - this is the WebRTC audio we need!
          phoneAudioStream = stream;
            console.log('[Audio] ‚úÖ phoneAudioStream SET:', stream.id);

            // CRITICAL: Pre-create audio context and connect to stream NOW (while fresh)
            // But don't initialize Soniox WebSocket yet - wait for bridge
            if (window.inboundCallId && !window.preConnectedAudioContext) {
              console.log('[Audio] üé§ PRE-CONNECTING to audio stream (before bridge)...');
              console.log('[Audio] Stream tracks:', stream.getAudioTracks().map(t => ({
                id: t.id,
                label: t.label,
                enabled: t.enabled,
                muted: t.muted,
                readyState: t.readyState
              })));

              window.preConnectedAudioContext = new AudioContext({ sampleRate: 16000 });
              console.log('[Audio] AudioContext created - State:', window.preConnectedAudioContext.state, 'Sample rate:', window.preConnectedAudioContext.sampleRate);

              window.preConnectedAudioSource = window.preConnectedAudioContext.createMediaStreamSource(stream);
              console.log('[Audio] MediaStreamSource created');
              console.log('[Audio] ‚úÖ Audio context pre-connected to live stream');
              console.log('[Audio] Will initialize Soniox WebSocket after bridge...');
            }
          
          console.log('[Audio] Remote party stream received:', {
            id: stream.id,
            active: stream.active,
            audioTracks: stream.getAudioTracks().length
          });
          
          // Connect audio element but MUTE it (we don't want to play it in browser)
          // The audio goes to the AI bridge instead
          const audioElement = document.getElementById('audio-stream');
          if (audioElement) {
            audioElement.srcObject = stream;
            audioElement.muted = true; // MUTE - audio goes to AI, not speakers!
            audioElement.play().catch(e => console.log('[Audio] Autoplay prevented:', e));
            console.log('[Audio] ‚úÖ Phone audio muted in browser (routed to AI instead)');
            addStatus('üîá Phone audio muted (routing to AI)', 'success');
          }
          
            // Monitor incoming audio levels
            const monitorContext = new AudioContext();
            const monitorSource = monitorContext.createMediaStreamSource(stream);
            const monitorAnalyser = monitorContext.createAnalyser();
            monitorSource.connect(monitorAnalyser);

            const monitorData = new Uint8Array(monitorAnalyser.frequencyBinCount);
            setInterval(() => {
              monitorAnalyser.getByteFrequencyData(monitorData);
              const avg = monitorData.reduce((a, b) => a + b) / monitorData.length;
              if (avg > 5) {
                console.log('[Audio] Remote party speaking - level:', Math.round(avg));
              }
            }, 2000);
          
          const audioTrack = stream.getAudioTracks()[0];
          if (audioTrack) {
            const settings = audioTrack.getSettings();
            const capabilities = audioTrack.getCapabilities ? audioTrack.getCapabilities() : {};
            const constraints = audioTrack.getConstraints ? audioTrack.getConstraints() : {};
            
            console.log('Audio track:', audioTrack);
            console.log('Audio settings:', JSON.stringify(settings, null, 2));
            console.log('Audio capabilities:', JSON.stringify(capabilities, null, 2));
            console.log('Audio constraints:', JSON.stringify(constraints, null, 2));
            console.table(settings);
            console.table(capabilities);
            
            // Try to get sample rate from different sources
            const sampleRate = settings.sampleRate || 
                              (capabilities.sampleRate && capabilities.sampleRate.max) || 
                              '48000 (default)';
            const channels = settings.channelCount || 1;
            
            addStatus(`üéµ Audio: ${sampleRate}Hz, ${channels} channel(s)`, 'info');
            
            // Also try to get info from AudioContext
            const audioElement = document.getElementById('audio-stream');
            if (audioElement && audioElement.srcObject) {
              const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
              addStatus(`üéµ AudioContext sample rate: ${audioCtx.sampleRate}Hz`, 'info');
            }
          }
        },
        onWSClose: () => {
          console.debug("onWSClose");
          addStatus('WebSocket connection closed', 'error');
        },
        onMessage: (verto, call, type) => {
          console.debug("onMessage", type);
          addStatus(`Message received: ${type.name}`, 'info');

          if (type.name === "clientReady") {
            // Check if this is an inbound call (webhook/headless mode)
            if (window.inboundCallId) {
              addStatus('‚úÖ Client ready - answering inbound call...', 'info');
              console.log('[Call] Inbound mode - answering call:', window.inboundCallId);
              
              // Answer the inbound call directly (no need to call "park")
              // The call is already active, we just need to connect to it
              verto.newCall({
                caller_id_name: null,
                caller_id_number: null,
                destination_number: "park", // Still need this to establish WebRTC
                useStereo: false,
                useVideo: false
              });
            } else {
              // Outbound mode - create call to park as usual
              addStatus('Client ready, creating call to "park"...', 'info');
              verto.newCall({
                caller_id_name: null,
                caller_id_number: null,
                destination_number: "park",
                useStereo: false,
                useVideo: false
              });
            }
          }
          
          // When we get "display" event, it typically means call progress/answered
            // For inbound calls, we handle initialization in onRemoteStream instead
            if (type.name === "display" && !window.inboundCallId && !elevenLabsBridge && phoneAudioStream) {
              addStatus('üìû Call appears to be answered (outbound mode)', 'success');
            
            // Start recording immediately when call is answered
            if (currentCallId) {
              startRecording(currentCallId);
            }
            
              // Note: For inbound calls, AI is initialized in onRemoteStream
          }
        },
        onDialogState: async (call) => {
          console.log("onDialogState", call);
          
          // Format call state nicely
          const stateInfo = {
            state: call.state,
            direction: call.direction,
            callId: call.callID
          };
          
          addStatus(`Call state: ${JSON.stringify(stateInfo, null, 2)}`, 'info');

          if (call.state === $.verto.enum.state.active) {
            // Store the call ID for later use (recording, etc.)
            currentCallId = call.params.variables.verto_svar_api_callid;
            console.log('[Call] Stored call ID:', currentCallId);
            
            // Check if this is an inbound call (headless webhook mode)
            if (window.inboundCallId && !window.inboundCallProcessed) {
              window.inboundCallProcessed = true; // Prevent duplicate processing
              
              addStatus('‚úÖ Park call active! Now answering and bridging inbound call...', 'success');
              console.log('[Call] Inbound mode - Park call ID:', currentCallId);
              console.log('[Call] Inbound call to answer:', window.inboundCallId);
              
                // Wait for AI to be ready before answering and bridging
              setTimeout(async () => {
                  // Step 1: Answer the inbound call NOW (agent is ready)
                  console.log('[Call] AI ready! Answering inbound call now...');
                  addStatus('ü§ñ AI ready - answering call...', 'success');

                  await fetch(`/proxy/telnect/api/v1/Calls/${window.inboundCallId}`, {
                method: "POST",
                headers: {
                  "Authorization": "Bearer " + telnectToken,
                  "Content-Type": "application/json"
                },
                body: JSON.stringify({
                  "actions": [{
                    "action": "answer"
                  }]
                })
                  });

                  console.log('[Call] ‚úÖ Call answered');
                  addStatus('‚úÖ Call answered - caller connected', 'success');

                  // Wait briefly for answer to complete
                   await new Promise(resolve => setTimeout(resolve, 200));

                  // Step 2: Bridge the two calls together
                  console.log('[Call] Bridging calls...');
                  fetch(`/proxy/telnect/api/v1/Calls/${window.inboundCallId}`, {
                      method: "POST",
                      headers: {
                        "Authorization": "Bearer " + telnectToken,
                        "Content-Type": "application/json"
                      },
                      body: JSON.stringify({
                        "actions": [{
                          "action": "bridge",
                          "param": {
                            "id": currentCallId  // Bridge with our park call
                          }
                        }]
                      })
              })
              .then(async res => {
                if (res && res.ok) {
                  console.log('[Call] ‚úÖ Calls bridged together!');
                        addStatus('‚úÖ Calls bridged! Audio should now be flowing...', 'success');
                  
                  // Now use the park call ID for recording
                  currentCallId = currentCallId; // Keep using park call ID
                  
                  // Start recording
                  startRecording(currentCallId);
                  
                        // CRITICAL: Wait for audio to flow through bridge, THEN initialize AI
                        console.log('[Call] Waiting 700ms for bridge audio to flow...');
                        setTimeout(() => {
                          console.log('[Call] ‚è∞ Bridge should be active, initializing AI now...');
                          if (phoneAudioStream && !sonioxSTT) {
                  initializeElevenLabsBridge();
                          } else {
                            console.warn('[Call] ‚ö†Ô∏è Cannot initialize - phoneAudioStream:', !!phoneAudioStream, 'sonioxSTT exists:', !!sonioxSTT);
                          }
                        }, 700); // Balanced timing for speed + stability
                } else {
                  const errorText = await res.text();
                  console.error('[Call] Bridge failed:', res.status, errorText);
                  throw new Error(`Failed to bridge calls: ${res.status} - ${errorText}`);
                }
              })
              .catch(err => {
                console.error('[Call] Error in inbound call flow:', err);
                addStatus('‚ùå Failed to bridge inbound call: ' + err.message, 'error');
              });
                }, 400); // Balanced timing for speed + stability
              
            } else {
              // Outbound call mode (disabled - only use inbound webhook mode)
              console.log('[Call] Outbound mode disabled - this should only run in inbound webhook mode');
              addStatus('‚ö†Ô∏è Outbound mode not configured', 'info');
              
              /* Disabled outbound calling
              fetch("/proxy/api/dial/easytelecom-v1/call", {
                method: "POST",
                headers: {
                  "Authorization": "Bearer " + salesysToken,
                  "Content-type": "application/json"
                },
                body: JSON.stringify({
                  "destinationPhoneNumber": numberToCall,
                  "callerPhoneNumber": numberToCallFrom,
                  "easyTelecomUserCallId": currentCallId
                })
              })
              .then(res => {
                if (res.ok) {
                  addStatus(`‚úì Call initiated to ${numberToCall}`, 'success');
                  addStatus(`‚è≥ Waiting for remote party to answer before starting AI agent...`, 'info');
                  
                  // DON'T initialize yet - wait for the call to be answered
                  // We'll do this in the onMessage handler when we see "display" event
                } else {
                  addStatus(`Failed to initiate call: ${res.status}`, 'error');
                }
              })
              .catch(err => {
                addStatus(`Error initiating call: ${err.message}`, 'error');
              });
              */
            }
          }
          
          // Terminate ElevenLabs agent and stop recording on hangup
          if (call.state === $.verto.enum.state.hangup || call.state === $.verto.enum.state.destroy) {
            addStatus('üìû Call ended, stopping recording and terminating AI agent...', 'info');
            console.log('[Call] Call ended - cleaning up resources');
            
            // Stop recording
            if (currentCallId) {
              stopRecording(currentCallId);
              currentCallId = null; // Clear the call ID
            }

              // Terminate Soniox STT
              if (sonioxSTT) {
                try {
                  console.log('[Call] Stopping Soniox STT...');
                  await sonioxSTT.stop();
                  addStatus('‚úì Soniox STT stopped', 'success');
                  sonioxSTT = null;
                } catch (err) {
                  console.error('Error stopping Soniox:', err);
                  addStatus(`‚ö†Ô∏è Error stopping Soniox: ${err.message}`, 'error');
                }
              }
            
            // Terminate AI agent immediately
            if (elevenLabsBridge) {
              try {
                console.log('[Call] Terminating AI agent...');
                
                // Close WebSocket immediately
                if (elevenLabsBridge.ws) {
                  elevenLabsBridge.ws.close();
                  elevenLabsBridge.ws = null;
                }
                
                  // Stop audio processing (only if in audio mode)
                if (elevenLabsBridge.phoneProcessor) {
                  elevenLabsBridge.phoneProcessor.disconnect();
                  elevenLabsBridge.phoneProcessor = null;
                }
                
                // End session
                await elevenLabsBridge.endSession();
                addStatus('‚úì AI agent terminated successfully', 'success');
                elevenLabsBridge = null;
              } catch (err) {
                console.error('Error terminating AI agent:', err);
                addStatus(`‚ö†Ô∏è Error terminating AI agent: ${err.message}`, 'error');
              }
            }
            
            // Reset state for next call
            window.inboundCallProcessed = false;
            window.elevenLabsInitialized = false;
            phoneAudioStream = null;
            
            // Signal Puppeteer to close this tab (for autonomous operation)
            if (window.inboundCallId) {
              console.log('[Call] Signaling tab cleanup to Puppeteer...');
              document.title = 'CALL_ENDED';
              
              // Give Puppeteer time to detect the title change before potential page unload
              setTimeout(() => {
                console.log('[Call] Cleanup signal sent, tab ready for closure');
              }, 500);
            }
          }
        },
        onEvent: (verto, event, data) => {
          console.debug("onEvent", event, data);
          addStatus(`Event: ${event}`, 'info');
        },
        onWSLogin: () => {
          console.debug("onWSLogin");
          addStatus('‚úì WebSocket logged in successfully', 'success');
        }
      });

      addStatus('Attempting Verto login...', 'info');
      verto.login();

    })
    .catch(err => {
      addStatus(`‚ùå Error: ${err.message}`, 'error');
      addStatus('This is likely a CORS issue. The API at app.salesys.se needs to allow requests from your origin.', 'error');
      console.error(err);
    });

    // Diagnostics
    function logAudioDiagnostics() {
      console.log('=== AUDIO DIAGNOSTICS ===');
      console.log('Phone stream:', phoneAudioStream);
      console.log('Phone stream tracks:', phoneAudioStream?.getTracks());
      console.log('Virtual audio source:', virtualAudioSource);
      console.log('Virtual stream:', virtualAudioSource?.getMediaStream());
      console.log('Virtual stream tracks:', virtualAudioSource?.getMediaStream().getTracks());
      console.log('ElevenLabs bridge:', elevenLabsBridge);
      console.log('Virtual audio queue length:', virtualAudioSource?.queueLength);
      console.log('Virtual audio playing:', virtualAudioSource?.playing);
      console.log('========================');
    }

    // Make diagnostics available globally
    window.logAudioDiagnostics = logAudioDiagnostics;

    // Audio bridge workers
    let phoneCaptureProcessor = null;
    let aiOutputCaptureNode = null;
    let aiAudioContext = null;

    // Function to initialize ElevenLabs using CUSTOM WebSocket bridge (headless)
    async function initializeElevenLabsBridge() {
      // Prevent multiple AI agent initializations
      if (window.elevenLabsInitialized) {
        console.log('[ElevenLabs] Already initialized, skipping duplicate initialization');
        return;
      }
      window.elevenLabsInitialized = true;
      
      if (!phoneAudioStream) {
        addStatus('‚ö†Ô∏è Phone audio stream not available', 'error');
        return;
      }

      if (!elevenLabsConfig.agentId || elevenLabsConfig.agentId === 'YOUR_AGENT_ID_HERE') {
        addStatus('‚ö†Ô∏è ElevenLabs agent ID not configured', 'error');
        return;
      }

      try {
          // ========================================
          // NEW ARCHITECTURE: Soniox STT + ElevenLabs Text
          // ========================================

          addStatus('ü§ñ Initializing NEW ARCHITECTURE: Soniox STT + ElevenLabs...', 'info');
          console.log('=== NEW ARCHITECTURE: Phone ‚Üí Soniox STT ‚Üí ElevenLabs (text) ‚Üí Phone ===');

          // STEP 1: Initialize Soniox STT for transcription
          console.log('[STT] ========== INITIALIZING SONIOX STT ==========');
          console.log('[STT] phoneAudioStream available:', !!phoneAudioStream);
          console.log('[STT] phoneAudioStream ID:', phoneAudioStream?.id);
          console.log('[STT] phoneAudioStream active:', phoneAudioStream?.active);
          console.log('[STT] phoneAudioStream tracks:', phoneAudioStream?.getAudioTracks().length);
          console.log('[STT] Soniox API key configured:', !!sonioxConfig.apiKey);

          if (!phoneAudioStream) {
            console.error('[STT] ‚ùå NO PHONE AUDIO STREAM! Cannot initialize Soniox');
            addStatus('‚ùå No phone audio stream available', 'error');
            throw new Error('Phone audio stream not available');
          }

          console.log('[STT] ‚úÖ Using REAL phoneAudioStream:', phoneAudioStream.id);

          addStatus('üé§ Starting Soniox STT with CORRECTED API...', 'info');
          sonioxSTT = new SonioxSTT({
            apiKey: sonioxConfig.apiKey,
            // Add context to improve transcription accuracy
            context: `
            Konversationen √§r ett inkommande kundservicesamtal till Fello, ett mobiloperat√∂rsbolag som erbjuder mobilabonnemang och relaterade tj√§nster. Kunden ringer in till Fellos kundservice och pratar med Fellippa, en digital assistent. Samtalet kan handla om att k√∂pa nya abonnemang, fakturafr√•gor, teknisk support, vidarekoppling till m√§nsklig support via BankID-identifiering, eller problem med surf och roaming. Kunden kan beh√∂va ange sitt personnummer f√∂r identifiering. Vanliga abonnemangsstorlekar √§r fem gigabyte, tio gigabyte, tjugo gigabyte, fyrtio gigabyte och hundra gigabyte. Priser inkluderar kampanjpris p√• tjugofem kronor i fyra m√•nader, samt ordinarie priser som hundratjugo kronor, etthundra√•ttio kronor, tv√•hundratretti kronor, tv√•hundranittio kronor och trehundrasjuttio kronor per m√•nad.
Fello, Fellippa, Felloservice, Telia, Telenor, Tre, Hallon, Comviq, Vimla, Halebop, Telmore, Tele2, Lebara, Lycamobile, Telianet.
abonnemang, mobilabonnemang, telefonabonnemang, mobilt bredband, gigabyte, gig, GB, surf, surfa, surfm√§ngd, surfpott, m√•nadssurf, extrasurf, dataroaming, roaming, femG, 5G, fyraG, 4G, treG, 3G, simkort, eSIM, personnummer, BankID, Bank-ID, Bank-IDE, legitimera, identifiera, identifiering, mobilhj√§lpen, knappval, knapptoner, stj√§rntecken, stj√§rna, fyrkant, fyrkanten, m√§nniska, handl√§ggare, nummerflytt, nummerportering, kortbetalning, autogiro, efaktura, faktura, m√•nadsfaktura, p√•minnelse, p√•minnelsefaktura, f√∂rfallodag, m√•nadsavgift, kampanjpris, bindningstid, upps√§gning, √•ngerr√§tt, driftst√∂rning, t√§ckning, mobilt√§ckning, n√§tt√§ckning, mobiln√§t, n√§tverksoperat√∂r, utomlands, utlandet, utlandssamtal, utlandstrafik, teknisk support, kundservice, kundtj√§nst, fels√∂kning,
nittonhundra, nittonhundraett, nittonhundratv√•, nittonhundratre, nittonhundrafyra, nittonhundrafem, nittonhundrasex, nittonhundrasju, nittonhundra√•tta, nittonhundranijo, nittonhundrati√≥, nittonhundraelva, nittonhundratolv, nittonhundratretton, nittonhundrafjorton, nittonhundrafemton, nittonhundrasexton, nittonhundrasjutton, nittonhundraarton, nittonhundranitton, nittonhundratjugo, nittonhundratjugoett, nittonhundratjugotv√•, nittonhundratjugotre, nittonhundratjugofyra, nittonhundratjugofem, nittonhundratjugosex, nittonhundratjugosju, nittonhundratjugo√•tta, nittonhundratjugonijo, nittonhundratretti, nittonhundratrettiett, nittonhundratrettitv√•, nittonhundratrettitre, nittonhundratrettifyra, nittonhundratrettifem, nittonhundratrettisex, nittonhundratrettisju, nittonhundratretti√•tta, nittonhundratrettinijo, nittonhundraf√∂rti, nittonhundraf√∂rtiett, nittonhundraf√∂rtitv√•, nittonhundraf√∂rtitre, nittonhundraf√∂rtifyra, nittonhundraf√∂rtifem, nittonhundraf√∂rtisex, nittonhundraf√∂rtisju, nittonhundraf√∂rti√•tta, nittonhundraf√∂rtinijo, nittonhundrafemti, nittonhundrafemtiett, nittonhundrafemtitv√•, nittonhundrafemtitre, nittonhundrafemtifyra, nittonhundrafemtifem, nittonhundrafemtisex, nittonhundrafemtisju, nittonhundrafemti√•tta, nittonhundrafemtinijo, nittonhundrasexti, nittonhundrasextiett, nittonhundrasextitv√•, nittonhundrasextitre, nittonhundrasextifyra, nittonhundrasextifem, nittonhundrasextisex, nittonhundrasextisju, nittonhundrasexti√•tta, nittonhundrasextinijo, nittonhundrasjutti, nittonhundrasjuttiett, nittonhundrasjuttitv√•, nittonhundrasjuttitre, nittonhundrasjuttifyra, nittonhundrasjuttifem, nittonhundrasjuttisex, nittonhundrasjuttisju, nittonhundrasjutti√•tta, nittonhundrasjuttinijo, nittonhundra√•tti, nittonhundra√•ttiett, nittonhundra√•ttitv√•, nittonhundra√•ttitre, nittonhundra√•ttifyra, nittonhundra√•ttifem, nittonhundra√•ttisex, nittonhundra√•ttisju, nittonhundra√•tti√•tta, nittonhundra√•ttinijo, nittonhundranitti, nittonhundranittiett, nittonhundranittitv√•, nittonhundranittitre, nittonhundranittifyra, nittonhundranittifem, nittonhundranittisex, nittonhundranittisju, nittonhundranitti√•tta, nittonhundranittinijo,
tjugohundra, tjugohundraett, tjugohundratv√•, tjugohundratre, tjugohundrafyra, tjugohundrafem, tjugohundrasex, tjugohundrasju, tjugohundra√•tta, tjugohundranijo, tjugohundrati√≥, tjugohundraelva, tjugohundratolv, tjugohundratretton, tjugohundrafjorton, tjugohundrafemton, tjugohundrasexton, tjugohundrasjutton, tjugohundraarton, tjugohundranitton, tjugohundratjugo, tjugohundratjugoett, tjugohundratjugotv√•, tjugohundratjugotre, tjugohundratjugofyra, tjugohundratjugofem,
nittinijo, nittio, nittiett, nittitv√•, nittitre, nittifyra, nittifem, nittisex, nittisju, nitti√•tta, √•ttinijo, √•ttio, √•ttiett, √•ttitv√•, √•ttitre, √•ttifyra, √•ttifem, √•ttisex, √•ttisju, √•tti√•tta, sjuttinijo, sjuttio, sjuttiett, sjuttitv√•, sjuttitre, sjuttifyra, sjuttifem, sjuttisex, sjuttisju, sjutti√•tta, sextinijo, sextio, sextiett, sextitv√•, sextitre, sextifyra, sextifem, sextisex, sextisju, sexti√•tta, femtinijo, femtio, femtiett, femtitv√•, femtitre, femtifyra, femtifem, femtisex, femtisju, femti√•tta, f√∂rtinijo, f√∂rtio, f√∂rtiett, f√∂rtitv√•, f√∂rtitre, f√∂rtifyra, f√∂rtifem, f√∂rtisex, f√∂rtisju, f√∂rti√•tta, trettinijo, trettio, trettiett, trettitv√•, trettitre, trettifyra, trettifem, trettisex, trettisju, tretti√•tta, tjugonijo, tjugo, tjugoett, tjugotv√•, tjugotre, tjugofyra, tjugofem, tjugosex, tjugosju, tjugo√•tta, nitton, arton, sjutton, sexton, femton, fjorton, tretton, tolv, elva, ti√≥, nijo, √•tta, sju, sex, fem, fyra, tre, tv√•, ett,
nollnoll, nollett, nolltv√•, nolltre, nollfyra, nollfem, nollsex, nollsju, noll√•tta, nollnijo
          `.trim(),
          onTranscript: (text) => {
            // Called when complete transcript is ready (after endpoint detection)
            console.log('[STT‚ÜíAI] ‚úÖ‚úÖ‚úÖ Complete transcript ready:', text);
            addStatus(`üë§ User said: "${text}"`, 'info');

            // Skip if this was just an interruption marker
            if (text === '---') {
              console.log('[STT‚ÜíAI] Skipping interrupt marker (---)');
              window.userInterruptionDetected = false;
              return;
            }

            // CRITICAL FIX: If we just sent an interrupt, skip this transcript
            // The user was speaking DURING the interruption, so we already notified the AI
            if (window.userInterruptionDetected) {
              console.log('[STT‚ÜíAI] ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è SKIPPING - This transcript was captured during interruption');
              console.log('[STT‚ÜíAI] Not sending to prevent duplicate AI responses');
              window.userInterruptionDetected = false;
              return;
            }

            // Play thinking sound IMMEDIATELY (while AI processes)
            playThinkingSound();

            // Send to ElevenLabs as text message
            if (elevenLabsBridge) {
              console.log('[STT‚ÜíAI] Sending to ElevenLabs:', text);
              elevenLabsBridge.sendTextMessage(text);
            } else {
              console.error('[STT‚ÜíAI] ‚ùå ElevenLabs bridge not available!');
            }
          },
          onPartialTranscript: (text) => {
            // Called for partial/interim results
            console.log('[STT] üìù Partial transcript:', text);
            addStatus(`üó£Ô∏è Hearing: "${text}"`, 'info');
            
            // ADAPTIVE INTERRUPTION DETECTION
            const aiIsPlaying = virtualAudioSource && (virtualAudioSource.playing || virtualAudioSource.queueLength > 0);
            
            console.log('[STT] Partial - AI playing:', aiIsPlaying, '(queue:', virtualAudioSource?.queueLength, ') detected:', window.userInterruptionDetected);
            
            if (text && text.trim().length > 0 && aiIsPlaying && !window.userInterruptionDetected) {
              // Get current adaptive sensitivity config
              const config = interruptionAnalytics.getConfig();
              
              // Count words in partial transcript
              const words = text.trim().split(/\s+/).length;
              
              console.log('[Adaptive] User speaking - words:', words, 'threshold:', config.minWords, 'debounce:', config.debounce + 'ms');
              
              // Clear existing timer
              if (interruptionDebounceTimer) {
                clearTimeout(interruptionDebounceTimer);
              }
              
              // Check if meets minimum word threshold
              if (words < config.minWords) {
                console.log('[Adaptive] ‚è∏Ô∏è Below word threshold, waiting...');
                return;
              }
              
              // Start debounce timer
              interruptionDebounceTimer = setTimeout(() => {
                // Double-check AI is still playing
                const stillPlaying = virtualAudioSource && (virtualAudioSource.playing || virtualAudioSource.queueLength > 0);
                
                if (stillPlaying && !window.userInterruptionDetected) {
                  window.userInterruptionDetected = true;
                  
                  // Calculate how long AI had been speaking
                  const aiDuration = window.aiSpeakingStartTime ? Date.now() - window.aiSpeakingStartTime : 0;
                  
                  console.log('[Interruption] üõëüõëüõë USER INTERRUPTED AI!');
                  console.log('[Interruption] AI had spoken for:', aiDuration, 'ms');
                  console.log('[Interruption] Queue had', virtualAudioSource.queueLength, 'chunks');
                  console.log('[Interruption] Sensitivity:', interruptionAnalytics.currentSensitivity);
                  addStatus('üõë User interrupted - stopping AI', 'info');
                  
                  // Record for analytics
                  interruptionAnalytics.record(aiDuration);
                  
                  // IMMEDIATELY stop AI audio
                  virtualAudioSource.clearBuffer();
                  console.log('[Interruption] ‚úÖ Audio buffer cleared');
                  
                  // Play interrupt sound
                  playInterruptSound();
                  
                  // Send interrupt marker to AI
                  if (elevenLabsBridge) {
                    console.log('[Interruption] Sending interrupt marker (---) to AI');
                    elevenLabsBridge.sendTextMessage('---');
                  }
                  
                  window.isAISpeaking = false;
                  window.aiSpeakingStartTime = null;
                  console.log('[Interruption] ‚úÖ‚úÖ‚úÖ Interruption complete');
                }
              }, config.debounce);
            } else if (!aiIsPlaying && interruptionDebounceTimer) {
              // AI stopped playing, cancel pending interrupt
              clearTimeout(interruptionDebounceTimer);
              interruptionDebounceTimer = null;
            }
          },
            onStatusChange: (status) => {
              console.log('[Soniox] Status:', status);
              if (status === 'connected') {
                addStatus('‚úì Soniox STT connected', 'success');
              } else if (status === 'error') {
                addStatus('‚ùå Soniox error', 'error');
              }
            },
            onError: (error) => {
              console.error('[Soniox] Error:', error);
              addStatus(`‚ùå Soniox error: ${error.message}`, 'error');
            }
          });

          // Initialize Soniox with phone audio stream (using correct API now)
          await sonioxSTT.initialize(
            phoneAudioStream,
            window.preConnectedAudioContext,
            window.preConnectedAudioSource
          );
          addStatus('‚úÖ Soniox STT active with CORRECT API endpoint', 'success');

          // STEP 2: Initialize ElevenLabs in TEXT INPUT mode
          addStatus('ü§ñ Starting ElevenLabs in TEXT mode...', 'info');
        elevenLabsBridge = new ElevenLabsBridge({
          agentId: elevenLabsConfig.agentId,
          apiKey: elevenLabsConfig.apiKey,
          virtualAudioSource: virtualAudioSource,
            useTextInput: true, // NEW: Enable text input mode
          customVariables: {
            call_id: window.inboundCallId || currentCallId || 'unknown',
            user_number: window.inboundCallId ? 
              (new URLSearchParams(window.location.search).get('caller') || 'unknown') : 
              (new URLSearchParams(window.location.search).get('called') || 'unknown'),
            caller_number: new URLSearchParams(window.location.search).get('caller') || 'unknown',
            called_number: new URLSearchParams(window.location.search).get('called') || 'unknown'
          },
          onStatusChange: (status, error) => {
              console.log('[ElevenLabs] Status:', status);
            if (status === 'websocket_connected') {
                addStatus('‚úì ElevenLabs WebSocket connected', 'success');
            } else if (status === 'conversation_ready') {
                addStatus('‚úÖ ElevenLabs ready for text messages!', 'success');
            } else if (status === 'connected') {
              addStatus('‚úì ElevenLabs connected', 'success');
            } else if (status === 'error') {
                addStatus(`‚ùå ElevenLabs error: ${error?.message}`, 'error');
            } else if (status === 'error_disconnect') {
              addStatus('‚ùå AI agent disconnected unexpectedly - hanging up call', 'error');
              // Hangup the call via Telnect API
              if (currentCallId) {
                fetch(`/proxy/telnect/api/v1/Calls/${currentCallId}`, {
                  method: 'POST',
                  headers: {
                    'Authorization': 'Bearer ' + telnectToken,
                    'Content-Type': 'application/json'
                  },
                  body: JSON.stringify({
                    actions: [{ action: 'hangup' }]
                  })
                }).then(() => {
                  console.log('[Call] Hangup triggered due to AI disconnect');
                }).catch(err => {
                  console.error('[Call] Failed to hangup:', err);
                });
              }
            } else if (status === 'disconnected') {
              addStatus('Disconnected', 'info');
            }
          },
          onAgentMessage: (message) => {
            addStatus(`ü§ñ AI: ${message}`, 'info');
          },
          onAudioLevel: (direction, level) => {
            updateAudioLevel(direction, level);
          }
        });

        // Expose bridge globally for context updates API
        window.elevenLabsBridge = elevenLabsBridge;
          window.sonioxSTT = sonioxSTT;
          console.log('[Bridge] Bridges exposed globally');

          // Initialize ElevenLabs (no phone stream needed in text mode)
          await elevenLabsBridge.initialize();
          addStatus('‚úÖ NEW ARCHITECTURE ACTIVE!', 'success');
          console.log('[Bridge] ‚úÖ Phone ‚Üí Soniox STT ‚Üí ElevenLabs (text) ‚Üí Phone (audio)');

      } catch (error) {
        addStatus(`‚ùå Failed: ${error.message}`, 'error');
        console.error('[Bridge] Error:', error);
      }
    }

    // No longer needed - custom bridge handles everything
    async function startAudioBridge() {
      console.log('[Bridge] Audio bridge handled by ElevenLabsBridge class');

      // Monitor input audio levels (phone ‚Üí AI)
      const monitorInput = () => {
        if (!elevenLabsBridge || !elevenLabsBridge.getInputByteFrequencyData) return;
        
        try {
          const inputData = elevenLabsBridge.getInputByteFrequencyData();
          if (inputData) {
            const average = inputData.reduce((a, b) => a + b) / inputData.length;
            const level = Math.min(100, Math.round(average / 2.55));
            updateAudioLevel('phone-to-agent', level);
          }
        } catch (e) {
          // SDK method not ready yet
        }
      };
      setInterval(monitorInput, 100);

      // Monitor output audio levels and capture to route to phone
      const monitorAndCaptureOutput = () => {
        if (!elevenLabsBridge || !elevenLabsBridge.getOutputByteFrequencyData) return;
        
        try {
          const outputData = elevenLabsBridge.getOutputByteFrequencyData();
          if (outputData) {
            const average = outputData.reduce((a, b) => a + b) / outputData.length;
            const level = Math.min(100, Math.round(average / 2.55));
            updateAudioLevel('agent-to-phone', level);
            
            // If AI is speaking (level > threshold), add audio to virtual source
            if (level > 10) {
              // Generate silence as placeholder - the SDK handles actual audio
              // This just keeps the virtual audio stream alive
              if (virtualAudioSource && !virtualAudioSource.playing) {
                virtualAudioSource.addSilence(20);
              }
            }
          }
        } catch (e) {
          // SDK method not ready yet
        }
      };
      setInterval(monitorAndCaptureOutput, 100);

      addStatus('‚úì Audio bridge monitoring active', 'success');
      console.log('[Bridge] Monitoring audio levels from SDK');
      
      // Try to route SDK output to virtual audio
      // The SDK doesn't natively support custom output routing, so we'll need
      // to capture the audio element it creates
      setTimeout(() => {
        tryCaptureSdkAudioOutput();
      }, 1000);
    }

    function tryCaptureSdkAudioOutput() {
      // Find any audio elements the SDK might have created
      const audioElements = document.querySelectorAll('audio');
      console.log('[Bridge] Searching for SDK audio elements...', audioElements.length, 'total');
      
      let foundSdkAudio = false;
      
      audioElements.forEach((audio, index) => {
        console.log('[Bridge] Audio element #' + index + ':', {
          id: audio.id,
          hasSrcObject: !!audio.srcObject,
          tagName: audio.tagName
        });
        
        if (audio.srcObject && audio.id !== 'audio-stream') {
          console.log('[Bridge] ‚úÖ Found SDK audio element!', audio);
          foundSdkAudio = true;
          
          // Capture this audio and route to virtual source
          try {
            console.log('[Bridge] Audio element details:', {
              muted: audio.muted,
              volume: audio.volume,
              paused: audio.paused,
              srcObject: audio.srcObject
            });
            
            // Try to capture from the MediaStream directly instead of the audio element
            if (audio.srcObject && audio.srcObject.getAudioTracks) {
              console.log('[Bridge] Using MediaStream capture (better approach)');
              const stream = audio.srcObject;
              const captureContext = new AudioContext({ sampleRate: 48000 });
              
              // Resume context
              captureContext.resume().then(() => {
                console.log('[Bridge] Audio context state:', captureContext.state);
              });
              
              console.log('[Bridge] Stream details:', {
                id: stream.id,
                active: stream.active,
                tracks: stream.getAudioTracks().length,
                trackLabel: stream.getAudioTracks()[0]?.label
              });
              
              const sdkSource = captureContext.createMediaStreamSource(stream);
              console.log('[Bridge] Created MediaStreamSource from SDK output');
              
              // Also create an analyser to verify we're getting audio
              const analyser = captureContext.createAnalyser();
              sdkSource.connect(analyser);
              
              const processor = captureContext.createScriptProcessor(4096, 1, 1);
              
              // Debug: Check if analyser shows audio
              const testData = new Uint8Array(analyser.frequencyBinCount);
              setInterval(() => {
                analyser.getByteFrequencyData(testData);
                const avg = testData.reduce((a, b) => a + b) / testData.length;
                if (avg > 0) {
                  console.log('[Bridge] ‚úÖ ANALYSER DETECTING AUDIO! Level:', avg);
                }
              }, 500);
            
            let chunksSent = 0;
            let processorCallCount = 0;
            
            processor.onaudioprocess = (e) => {
              processorCallCount++;
              
              if (processorCallCount === 1) {
                console.log('[Bridge] ‚úÖ ScriptProcessor ACTIVE - audio routing started!');
              }
              
              const audioData = e.inputBuffer.getChannelData(0);
              
              // Check if there's actual audio
              const hasAudio = audioData.some(s => Math.abs(s) > 0.01);
              
              if (hasAudio) {
                chunksSent++;
                if (chunksSent === 1 || chunksSent % 10 === 0) {
                  console.log('[Bridge] üîä Routing AI audio chunk #' + chunksSent + ' to phone!');
                }
                
                // Convert to PCM16 and send to virtual audio
                const pcm16 = new Int16Array(audioData.length);
                for (let i = 0; i < audioData.length; i++) {
                  const s = Math.max(-1, Math.min(1, audioData[i]));
                  pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                }
                
                virtualAudioSource.addAudioData(pcm16);
              } else if (processorCallCount % 50 === 0) {
                console.log('[Bridge] Processor running but no audio detected yet...');
              }
            };
            
              console.log('[Bridge] Connecting audio nodes...');
              // Connect: Source ‚Üí Analyser ‚Üí Processor ‚Üí Destination
              analyser.connect(processor);
              processor.connect(captureContext.destination);
              
              console.log('[Bridge] Audio pipeline: MediaStream ‚Üí Analyser ‚Üí Processor ‚Üí Destination');
              console.log('[Bridge] Audio nodes connected successfully!');
              
              addStatus('‚úÖ SDK audio captured ‚Üí routing to phone!', 'success');
              console.log('[Bridge] üéß AI audio pipeline: MediaStream ‚Üí Virtual Audio ‚Üí Phone');
            } else {
              console.error('[Bridge] No srcObject.getAudioTracks available');
            }
          } catch (err) {
            console.error('[Bridge] ‚ùå Failed to capture SDK audio:', err);
            addStatus(`‚ö†Ô∏è Audio capture error: ${err.message}`, 'error');
          }
        }
      });
      
      if (!foundSdkAudio) {
        console.warn('[Bridge] No SDK audio element found yet, will retry...');
        setTimeout(tryCaptureSdkAudioOutput, 2000); // Retry in 2 seconds
      }
    }

    function stopAudioBridge() {
      console.log('[Bridge] Stopping audio bridge');
      // Cleanup handled by SDK
    }

    } // End of startBridge function

    /**
     * Start bridge with incoming call ID (for headless/webhook operation)
     * This is called by Puppeteer when a webhook arrives
     */
    window.startBridgeWithCallId = async function (inboundCallId) {
      console.log('[Bridge] üìû Starting bridge for inbound call:', inboundCallId);
      
      // Store the inbound call ID globally
      window.inboundCallId = inboundCallId;
      
      // Start the bridge (will use inbound call ID instead of creating new call)
      startBridge();
    };

    // Add button to run diagnostics
    console.log('üí° TIP: Run window.logAudioDiagnostics() to see detailed audio status');
  </script>
</body>


<script>


</script>


</html>